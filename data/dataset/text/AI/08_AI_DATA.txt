The blueprint for today's artificial intelligence, neural network and machine learning technology is the human brain, simply because it is the most effective tool for solving problems that we know of. Active on Facebook +6659 views in the last 24 hours Active on Facebook Active on LinkedIn Active on Facebook +31983 views in the last 24 hours However, there is a big part of the puzzle missing ' the aspect of human intelligence that we know as emotional intelligence, or empathy. Emotional intelligence is what allows us to take the feelings and considerations of other people into account in the solutions we make. And so far, progress here has been limited. Alexa, helpful as she may be in some circumstances, will not consider your feelings, or those of others affected by her actions, as she keeps your smart home running smoothly. But all that could be about to change ' in fact, has to change, if AI is to reach its potential as a tool for assisting in our business and day-to-day lives. Recently, emotion-focused AI developer Affectiva became one of the few small businesses to be asked to join the Partnership on AI to Benefit People and Society. The interest of the 'grand masters' of AI which make up the partnership ' Google, Microsoft, Facebook, etc ' in Affectiva's growing business is a sure sign that this overlooked aspect of AI is starting to get the attention it deserves. Affectiva co-founder and CEO Rana el Kaliouby talked to me about her company's work to develop what she calls 'multi-modal emotion AI'. There may already be a growing understanding of how sentiment analysis can help machines understand how humans are feeling, and adapt their behavior accordingly. But a great deal of valuable data which communicate our emotional state is lost if machines cannot also read our expressions, gestures, speech patterns, tone of voice or body language. 'There's research showing that if you're smiling and waving or shrugging your shoulders, that's 55% of the value of what you're saying ' and then another 38% is in your tone of voice. 'Only seven per cent is in the actual choice of words you're saying, so if you think about it like that, in the existing sentiment analysis market which looks at keywords and works out which specific words are being used on Twitter, you're only capturing 7% of how humans communicate emotion, and the rest is basically lost in cyberspace.' Affectiva's Emotion AI technology is already in use by 1,400 brands worldwide, including CBS, MARS and Kellogg's, many of whom use it to judge the emotional effect of adverts by asking viewers to switch on their cameras while video plays. The facial images are analysed with deep learning algorithms which accurately classify them according to the feelings of the viewer.
